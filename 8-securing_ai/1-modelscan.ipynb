{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eaa7fc6-8e46-4b1f-8c64-94c33e62b948",
   "metadata": {},
   "source": [
    "# Modelscan\n",
    "It's not just the code we want to check, we also want to make sure our model doesn't have any vurnabilities.  \n",
    "This can come from finetuning on an already exposed model or if you are using techniques that makes your model especially easy to steal data from.  \n",
    "Luckily, HuggingFace does some scanning, but they don't cover everything. You can read more about that [here](https://huggingface.co/docs/hub/en/security)\n",
    "\n",
    "Here we will use Modelscan, which is a great library for checking that there is no hidden code inside the model, among other things."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc54060a-827b-487a-91eb-0b1b98725722",
   "metadata": {},
   "source": [
    "First we install the package and silence some CUDA warnings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6169fdfc-2fac-4084-9c10-33a275c68ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install modelscan[tensorflow]\n",
    "%env TF_CPP_MIN_LOG_LEVEL=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c50081e-97d5-4398-948b-9d6281a1d3bb",
   "metadata": {},
   "source": [
    "Let's download a totally safe model from a nice [github page](https://github.com/rhoai-mlops/happy_safe_model) we found! ðŸ™Œ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb81d57-4fa5-4944-8be3-e345a8e71eb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget https://github.com/rhoai-mlops/happy-safe-model/raw/main/totally_safe_model.keras\n",
    "!modelscan -p totally_safe_model.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602d50bf-a189-4e12-8591-26464b6a7511",
   "metadata": {},
   "source": [
    "As we can see from the output, the model was detected as potentially dangerous ðŸ˜±  \n",
    "It got a severity of Medium and description: `Use of unsafe operator 'Lambda' from module 'Keras'`.  \n",
    "In this case, we sneaked in an additional layer in the model that can execute essentially any code we want.\n",
    "\n",
    "Now let's scan our model to make sure that one is at least safe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b64ffb0-ce5e-4a59-9872-756c52d7f5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!modelscan -p ../2-dev_datascience/models/jukebox/1/model.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4a7685-75c6-428e-9ab6-75d40d2af6bb",
   "metadata": {},
   "source": [
    "Phew.. No issues found with this one ðŸ˜…  \n",
    "We can also scan our model artifacts. Pickle files are particularily dangerous as they can package arbitrary code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff28d5-3228-47c2-aeb8-3939526c697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!modelscan -p ../2-dev_datascience/models/jukebox/1/artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10faf05f-3a62-4be1-bda8-0b7ab2cb0a14",
   "metadata": {},
   "source": [
    "Now that we have scanned our models, we can feel more confident in putting them in production!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
